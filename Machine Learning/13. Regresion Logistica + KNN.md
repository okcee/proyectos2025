# Punto de partida del ejercicio sólo con Regresión Logística

**Sugerencia: Podrías usar KNN con n_neighbors=3 como una herramienta de preprocesamiento o para ingeniería de características que luego se incorporarían a un modelo de regresión logística.**  

Revisa "Mi_Ejercicio", añadido al final y, analiza cómo realizar la sugerencia anterior y explica cómo afecta y qué diferencias hay en el resultado  

Mi_Ejercicio
```python
url = ("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data")
df = pd.read_csv(url, header=None)
df.columns = [
    "Sex",
    "Length",
    "Diameter",
    "Height",
    "Whole weight",
    "Shucked weight",
    "Viscera weight",
    "Shell weight",
    "Rings",
]

df2 = df.drop("Sex", axis=1) # Se cambia el nombre de la variable para que no afecte al dataframe original y cree una inconsistencia en los siguientes apartados del ejercicio

df2.describe()

X = df.drop('Sex', axis=1)
y = df['Sex']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=45)

modelo = LogisticRegression(max_iter=8000)
modelo.fit(X_train, y_train)

predicciones = modelo.predict(X_test)

cm = confusion_matrix(y_test, predicciones)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels="Sex")
disp.plot()
plt.title("Mi Matriz de Confusión")
plt.show()
```

# Resolución a la sugerencia Regresión Logística + KNN

```Python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import numpy as np

url = ("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data")
df = pd.read_csv(url, header=None)
df.columns = [
    "Sex",
    "Length",
    "Diameter",
    "Height",
    "Whole weight",
    "Shucked weight",
    "Viscera weight",
    "Shell weight",
    "Rings",
]
df2 = df.drop("Sex", axis=1).copy() # Se copia para evitar modificaciones en el original

# Preparación de los datos para el modelo de regresión logística original
X_original = df.drop('Sex', axis=1)
y_original = df['Sex']
X_train_original, X_test_original, y_train_original, y_test_original = train_test_split(X_original, y_original, test_size=0.4, random_state=45)

# Entrenar y evaluar el modelo de regresión logística original
modelo_original = LogisticRegression(max_iter=8000)
modelo_original.fit(X_train_original, y_train_original)
predicciones_original = modelo_original.predict(X_test_original)
accuracy_original = accuracy_score(y_test_original, predicciones_original)
cm_original = confusion_matrix(y_test_original, predicciones_original)

print("Resultados del modelo de regresión logística original:")
print(f"Accuracy: {accuracy_original:.4f}")
disp_original = ConfusionMatrixDisplay(confusion_matrix=cm_original, display_labels=modelo_original.classes_)
disp_original.plot()
plt.title("Matriz de Confusión (Regresión Logística Original)")
plt.show()

# --- Implementación de la sugerencia: KNN para ingeniería de características ---

# 1. Entrenar un modelo KNN con n_neighbors=3 en los datos de entrenamiento
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_original, y_train_original)

# 2. Predecir las probabilidades de clase para los conjuntos de entrenamiento y prueba usando KNN
#    Esto generará nuevas características basadas en la pertenencia a las clases predichas por KNN
knn_train_probs = knn.predict_proba(X_train_original)
knn_test_probs = knn.predict_proba(X_test_original)

# 3. Crear nuevos conjuntos de características combinando las características originales con las probabilidades de KNN
X_train_knn_features = np.concatenate((X_train_original, knn_train_probs), axis=1)
X_test_knn_features = np.concatenate((X_test_original, knn_test_probs), axis=1)

# 4. Entrenar un nuevo modelo de regresión logística utilizando las características aumentadas
modelo_knn_features = LogisticRegression(max_iter=8000)
modelo_knn_features.fit(X_train_knn_features, y_train_original)

# 5. Realizar predicciones con el nuevo modelo
predicciones_knn_features = modelo_knn_features.predict(X_test_knn_features)

# 6. Evaluar el rendimiento del nuevo modelo
accuracy_knn_features = accuracy_score(y_test_original, predicciones_knn_features)
cm_knn_features = confusion_matrix(y_test_original, predicciones_knn_features)

print("\nResultados del modelo de regresión logística con características de KNN (n_neighbors=3):")
print(f"Accuracy: {accuracy_knn_features:.4f}")
disp_knn_features = ConfusionMatrixDisplay(confusion_matrix=cm_knn_features, display_labels=modelo_knn_features.classes_)
disp_knn_features.plot()
plt.title("Matriz de Confusión (Regresión Logística con Características KNN)")
plt.show()

# --- Otra forma de usar KNN para ingeniería de características (vecinos como características) ---
# (Este es un ejemplo más complejo y puede no ser siempre beneficioso)

# from sklearn.neighbors import NearestNeighbors
#
# n_neighbors_knn_features = 3
# nn = NearestNeighbors(n_neighbors=n_neighbors_knn_features)
# nn.fit(X_train_original)
#
# # Para cada instancia, obtener los índices de sus k vecinos más cercanos en el conjunto de entrenamiento
# train_neighbors_indices = nn.kneighbors(X_train_original, return_distance=False)
# test_neighbors_indices = nn.kneighbors(X_test_original, return_distance=False)
#
# # Crear nuevas características basadas en la información de los vecinos (ejemplo: la moda de la clase de los vecinos)
# def get_neighbor_class_mode(indices, y_train):
#     neighbor_classes = y_train.iloc[indices].values
#     # Manejar el caso donde no hay una moda única (tomar el primero)
#     if len(np.unique(neighbor_classes)) == n_neighbors_knn_features:
#         return neighbor_classes[0]
#     else:
#         return pd.Series(neighbor_classes).mode()[0]
#
# X_train_neighbor_mode = np.array([get_neighbor_class_mode(idx, y_train_original) for idx in train_neighbors_indices])
# X_test_neighbor_mode = np.array([get_neighbor_class_mode(idx, y_train_original) for idx in test_neighbors_indices])
#
# # Codificar las nuevas características categóricas (si es necesario)
# from sklearn.preprocessing import LabelEncoder
# le = LabelEncoder()
# X_train_neighbor_mode_encoded = le.fit_transform(X_train_neighbor_mode).reshape(-1, 1)
# X_test_neighbor_mode_encoded = le.transform(X_test_neighbor_mode).reshape(-1, 1)
#
# # Combinar con las características originales
# X_train_neighbor_features = np.concatenate((X_train_original, X_train_neighbor_mode_encoded), axis=1)
# X_test_neighbor_features = np.concatenate((X_test_original, X_test_neighbor_mode_encoded), axis=1)
#
# # Entrenar un modelo de regresión logística con estas nuevas características
# modelo_neighbor_features = LogisticRegression(max_iter=8000)
# modelo_neighbor_features.fit(X_train_neighbor_features, y_train_original)
#
# # Evaluar el rendimiento
# predicciones_neighbor_features = modelo_neighbor_features.predict(X_test_neighbor_features)
# accuracy_neighbor_features = accuracy_score(y_test_original, predicciones_neighbor_features)
# cm_neighbor_features = confusion_matrix(y_test_original, predicciones_neighbor_features)
#
# print("\nResultados del modelo de regresión logística con característica de la moda de los vecinos (KNN n=3):")
# print(f"Accuracy: {accuracy_neighbor_features:.4f}")
# disp_neighbor_features = ConfusionMatrixDisplay(confusion_matrix=cm_neighbor_features, display_labels=modelo_neighbor_features.classes_)
# disp_neighbor_features.plot()
# plt.title("Matriz de Confusión (Regresión Logística con Moda de Vecinos KNN)")
# plt.show()
```

# Análisis de cómo realizar la sugerencia y sus efectos:

La sugerencia de usar KNN con n_neighbors=3 como una herramienta de preprocesamiento o para ingeniería de características implica los siguientes pasos:

Entrenar un modelo KNN: Se entrena un modelo KNN (un algoritmo de clasificación) utilizando los datos de entrenamiento originales (X_train_original, y_train_original) con el parámetro n_neighbors establecido en 3.

Generar nuevas características: Se utilizan las probabilidades de clase predichas por el modelo KNN para los conjuntos de entrenamiento y prueba. Para cada instancia, el modelo KNN con n_neighbors=3 devuelve un vector de probabilidades, donde cada elemento representa la probabilidad de que esa instancia pertenezca a cada una de las clases posibles ('F', 'I', 'M'). Estos vectores de probabilidad se consideran nuevas características.

Combinar características: Las nuevas características generadas por KNN se concatenan con las características originales (X_train_original y X_test_original). Esto crea conjuntos de datos aumentados (X_train_knn_features y X_test_knn_features) que ahora incluyen información sobre la clasificación basada en los 3 vecinos más cercanos.

Entrenar un modelo de regresión logística: Se entrena un nuevo modelo de regresión logística utilizando estos conjuntos de datos aumentados (X_train_knn_features, y_train_original). El objetivo es ver si la información proporcionada por el modelo KNN puede mejorar el rendimiento del modelo de regresión logística.

## Cómo afecta y qué diferencias hay en el resultado:

Aumento de la dimensionalidad: La principal diferencia en los datos es el aumento del número de características. Si originalmente tenías n características, después de agregar las probabilidades de KNN para un problema de clasificación con k clases, tendrás n + k características. En este caso, como la variable 'Sex' tiene 3 clases ('F', 'I', 'M'), se añadirán 3 nuevas características (las probabilidades de pertenecer a cada una de estas clases).  

Información adicional para la regresión logística: Las nuevas características proporcionadas por KNN introducen información no lineal sobre las relaciones entre las instancias. El modelo KNN captura la estructura local de los datos basada en la proximidad de los vecinos. Al incluir las probabilidades de clase de KNN como características, el modelo de regresión logística puede potencialmente aprender patrones más complejos que no podría capturar solo con las características originales.  

Posible mejora en el rendimiento: La inclusión de estas nuevas características podría llevar a una mejora en el rendimiento del modelo de regresión logística (mayor accuracy, precisión, recall, F1-score, etc.). Sin embargo, esto no está garantizado y depende de la naturaleza de los datos y la relación entre las características y la variable objetivo. En algunos casos, podría incluso empeorar el rendimiento debido a la introducción de ruido o redundancia.  

Cambio en la interpretación del modelo: La interpretación de los coeficientes del modelo de regresión logística se vuelve más compleja, ya que ahora están asociados a las características originales y a las nuevas características generadas por KNN. La importancia relativa de las características originales podría cambiar.  

Mayor complejidad computacional: El proceso implica entrenar un modelo KNN adicional y generar predicciones con él, lo que aumenta el tiempo de computación en comparación con entrenar solo un modelo de regresión logística.  

### En el código de ejemplo:  

El código implementa exactamente estos pasos. Primero, entrena y evalúa un modelo de regresión logística base utilizando solo las características originales. Luego, entrena un modelo KNN con n_neighbors=3, utiliza sus predicciones de probabilidad para generar nuevas características y entrena un segundo modelo de regresión logística con estas características aumentadas. Finalmente, compara el rendimiento (accuracy y matriz de confusión) de ambos modelos para observar el efecto de la ingeniería de características con KNN.  

Al ejecutar este código, podrás observar si la inclusión de las características generadas por KNN mejora o empeora el rendimiento del modelo de regresión logística para la tarea de clasificación del sexo de los abulones. Es importante tener en cuenta que el resultado puede variar y no siempre se observará una mejora. La efectividad de esta técnica depende mucho del dataset específico.  